{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN_pipeline.\n",
    "Код реализует алгоритм симуляции торгов. Формирует предсказания, используя уже обученные модели ChP_c, ChP_r и TF, и оценивает их качество. При этом, если на вход для получения предсказаний подаются сырые данные, к ним применяются процедуры преобработки и формирования признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузка нужных библиотек (встроенные библиотеки Анаконды)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('agg')#этот бэкэнд нужен чтобы переводить картинки сразу в RGB-матрицу\n",
    "from matplotlib import pyplot as plt #Источник pip install mpl_finance - раньше было частью matplotlib\n",
    "from mpl_finance import candlestick2_ohlc#модуль для рисования графиков со свечами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#библиотека для работы со сверточными сетями\n",
    "import cntk as C #Источник: https://docs.microsoft.com/en-us/cognitive-toolkit/Setup-Windows-Python\n",
    "#для работы CNTK требуется доустановка MKL-DNN - https://anaconda.org/anaconda/mkl-dnn\n",
    "#прогресс бар для контроля исполнения кода (актуально для длинных расчетов)\n",
    "import progressbar as pbar# Источник: https://anaconda.org/conda-forge/progressbar2\n",
    "import trend_functions as trd #может потребоваться установка xgboost - https://www.lfd.uci.edu/~gohlke/pythonlibs/#xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings;warnings.simplefilter('once')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Логика Pipeline\n",
    "\n",
    "Первой запускается модель (ChP_с), которая отвечает за определение наличия точек смены тенденции в окне шириной n_days (например,n_days=25 соответствует примерно 5 неделям, а n_days =75-15ти). Для снижения времени расчетов окна берутся с шагом skip (например, skip= 5 дней), что дает некритичную погрешность.  Если модель ChP_c определяет наличие смены тенденции в окне (ChP_c_pred=1), подключается модель ChP_r, для определения того, где именно началась последняя тенденция (win_srt). Если же значение ChP_c_pred=0 (не было смены тенденции), то точкой начала тенденции (win_srt) является значение, опреденное по предыдущему окну (или же начало файла данных). Концом тенденции (win_end) по умолчанию считается дата, на которую запускается модель ChP_c. Котировки акций, взятые за даты между win_srt и win_end передаются в модель TF,которая определяется тип тенденции (восходящий, нисходящий тренд или флэт) - так опредлеляется значение ScoredNewTypeBool на текущую дату. Оно дейстствует следующие skip дней, то есть до очередного запуска расчета.  Для Pipeline финальной является оценка, которую дала именно модель TF, поэтому после полной обработки определяются фактические точки смены тенденции ScoredNewTrigger. Промежуточные значения (точки win_start) сохраняются в переменной ScoredNewTrigger_tmp. \n",
    "\n",
    "Код по-разному работает для случаев, когда на вход подаются размеченные или неразмеченные данные, а также в зависимости от ряда других дополнительных параметров, которые устанавливаюся перед началом расчета.\n",
    "\n",
    "Pipeline требует предварительной подготовки данных с помощью ряда вспомогательных функций, приведенных ниже, а также сохраненных в модуле trend_functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_process_and_save(input_set,Trigger_col,dpi,n_days,skip,filename,norm,mode):\n",
    "    CNN_data,CNN_images,CNN_labels,CNN_im_shape,CNN_lb_shape =CNN_changes_preprocess_op4_fin(\n",
    "                                                                    input_set,Trigger_col,dpi,n_days,mode,skip)\n",
    "    CNN_trans=transpose_data(CNN_images,CNN_labels,mode)\n",
    "    savetxt_chP_op4_fin(os.path.join(path, filename), CNN_trans, CNN_lb_shape,norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_prices_volumes (input_set, logarithm):\n",
    "    ##\n",
    "    #В сохраненном файле вместо значений  Open,High,Low находятся их отношения к цене закрытия (логарифмированной или нет)\n",
    "    #Чтобы восстановить исходные значения мы делаем обратное преобразование\n",
    "    #Этого можно избежать, если изначально работать с сырыми данными, а не с сохранненным файлом\n",
    "    num_cols_orig=[\"Open_orig\",\"High_orig\",\"Low_orig\",\"Close_orig\", \"Volume_orig\"]\n",
    "    num_cols=[\"Open\",\"High\",\"Low\",\"Close\", \"Volume\"]\n",
    "    if logarithm: #для моделей, где использована разность логарифмов, а не частное\n",
    "        input_set[num_cols_orig]=np.exp(input_set[num_cols])\n",
    "    else:    \n",
    "        input_set[num_cols_orig]=input_set[num_cols]\n",
    "    \n",
    "    input_set[\"Open_orig\"] =input_set[\"Open_orig\"]*input_set[\"Close_orig\"]\n",
    "    input_set[\"High_orig\"] =input_set[\"High_orig\"]*input_set[\"Close_orig\"]\n",
    "    input_set[\"Low_orig\"] = input_set[\"Low_orig\"]*input_set[\"Close_orig\"]\n",
    "    \n",
    "    if logarithm: #если далее надо вернуться к логарифмам\n",
    "        input_set[num_cols_orig]= np.log(input_set[num_cols_orig])\n",
    "    input_set.drop(columns=num_cols, inplace=True)\n",
    "    input_set.rename(columns={\"Open_orig\": \"Open\", \"High_orig\": \"High\",\n",
    "                             \"Low_orig\":\"Low\",\"Close_orig\":\"Close\", \"Volume_orig\":\"Volume\"},\n",
    "                    inplace=True)\n",
    "    return input_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctf_parser (fname, headers):\n",
    "    parsed_data={}\n",
    "    with open(fname,'r') as f:\n",
    "        fst_line=f.readline()\n",
    "        fst_columns=fst_line.strip().split('|')\n",
    "        fst_headers=[fst_columns[i].split(\" \")[0] for i in range(len(fst_columns))]\n",
    "        indices=[fst_headers.index(h) for h in headers]\n",
    "        line_id=0\n",
    "    f=open(fname,'r')\n",
    "    for line in f:\n",
    "        columns=line.strip().split('|')\n",
    "        for ix in indices:\n",
    "            ix_header=columns[ix].split(\" \")[0]\n",
    "            ix_data=columns[ix].split(\" \")[1:]\n",
    "            parsed_data[(ix_header, line_id)]=np.array(ix_data).flatten()\n",
    "        line_id+=1\n",
    "    f.close()\n",
    "    return parsed_data, line_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctf_parser_light(fname, header):\n",
    "    parsed_data=[]\n",
    "    with open(fname,'r') as f:\n",
    "        fst_line=f.readline()\n",
    "        fst_columns=fst_line.strip().split('|')\n",
    "        fst_headers=[fst_columns[i].split(\" \")[0] for i in range(len(fst_columns))]\n",
    "        ix=fst_headers.index(header)\n",
    "    f=open(fname,'r')\n",
    "    for line in f:\n",
    "        columns=line.strip().split('|')\n",
    "        ix_data=columns[ix].split(\" \")[1:]\n",
    "        parsed_data.append(np.array(ix_data).flatten())\n",
    "    f.close()\n",
    "    return np.array(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_picture (price_set,dpi):\n",
    "    #отрисовываем картинку для каждого среза\n",
    "    fig, ax = plt.subplots(dpi=dpi)# выбираем нужное разрешение\n",
    "    plt.axis('off')#отключаем оси\n",
    "    fig.tight_layout(pad=0)#убираем рамку\n",
    "    #создаем график из свечей\n",
    "    candlestick2_ohlc(ax,price_set[\"Open\"],price_set[\"High\"],price_set[\"Low\"],price_set[\"Close\"],\n",
    "                                      colorup='b', colordown='r',width=1,alpha=1)#Цвета такие, чтобы в RGB было больше нулей\n",
    "    fig.canvas.draw()#отрисовываем картинку\n",
    "    #print(fig.canvas.get_width_height())\n",
    "\n",
    "    #получаем из картинки матрицу RGB в виде вектора\n",
    "    image_flat = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    #сохраняем размеры матрицы для картинки\n",
    "    im_shape=fig.canvas.get_width_height()[::-1] + (3,) #кортеж: высота,ширина, количество каналов\n",
    "    image=image_flat.reshape(im_shape)#превращаем в массив размером (height,width,3)\n",
    "    #сохраняем картинку в папку (необязательно, но так можно видеть, что вышло)\n",
    "#   plt.savefig(os.path.join(path,\"change_images_op2_\"+str(dpi)+\"dpi_\"+str(n_days)+\"d\",f+\"_\"+date+ln+str(CNN_labels[(f,date)])+'.png'), \n",
    "#       dpi=dpi,bbox_inches='tight',pad_inches=0,transparent=True)\n",
    "    plt.close(fig)\n",
    "    return image_flat,im_shape,image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_changes_preprocess_op4_fin(input_set,Trigger_col,dpi,n_days,mode=\"OUT\", skip=1):\n",
    "    \n",
    "    #Принимает на вход массив котировок (input_set), в колонке Trigger_col содержится метка о смене тенденции,\n",
    "    #dpi - разрешение, с которым сохраняются картинки\n",
    "    #Возвращает CNN_data - массив из оцифрованных картинок с метками\n",
    "    #CNN_images - только оцифрованные картинки,CNN_labels - только метки,\n",
    "    #CNN_im_shape - размер одной картинки в формате \"высота,ширина, количество каналов\"\n",
    "    #CNN_lb_shape- размер одной метки\n",
    "    \n",
    "    input_set.reset_index(drop=True,inplace=True)\n",
    "    matplotlib.pyplot.close('all')\n",
    "    \n",
    "    CNN_data={}\n",
    "    CNN_images={}\n",
    "    CNN_labels={}\n",
    "    \n",
    "  \n",
    "    CNN_im_shape=(0,0,0)\n",
    "    CNN_lb_shape=1\n",
    "    trigger=0# по умолчанию для неразмеченных данных\n",
    "    #каждая метка содержит 1 значение - была или нет смена тенденции\n",
    "    srange=list(input_set['Source'].drop_duplicates())\n",
    "    print (\"Started processing images from \", str(len(srange)), \" sources\")\n",
    "    for s in srange:#для каждого источника \n",
    "        stock_set=input_set[input_set['Source']==s]#выбрали источник\n",
    "        frange=list(stock_set['Stockname'].drop_duplicates())#составляем список акций в этом источнике\n",
    "        bar = pbar.ProgressBar(max_value=len(frange)).start() # Создаём новый progress bar\n",
    "        t=0\n",
    "\n",
    "        for f in frange:# перебираем все файлы\n",
    "            TrendNew=stock_set[stock_set[\"Stockname\"]==f]\n",
    "            start_ix=TrendNew.index[0]\n",
    "            for start_ix in TrendNew.index[:-(n_days-1):skip]:\n",
    "                ChangeWin=TrendNew.loc[start_ix:start_ix+n_days-1]\n",
    "                date_srt=str(ChangeWin.loc[start_ix, \"Date\"].date())\n",
    "                date_end=str(ChangeWin.loc[start_ix+n_days-1, \"Date\"].date())\n",
    "                \n",
    "                #заполняем метки\n",
    "                if mode==\"OUT\":\n",
    "                    trigger=int(np.max(ChangeWin[Trigger_col]))# 1 если была смена тенденции, 0 -если нет\n",
    "                    CNN_labels[(s,f,date_srt,date_end)]=trigger\n",
    "                #отрисовываем картинку для каждого среза\n",
    "                image_flat,CNN_im_shape,image=get_picture(ChangeWin,dpi)\n",
    "                \n",
    "                #сохраняем в словарь картинку в виде матрицы с заданным размером\n",
    "                CNN_images[(s,f,date_srt,date_end)]= image\n",
    "\n",
    "                #сохраняем в словарь данные - плоская картинка + метка \n",
    "                CNN_data[(s,f,date_srt,date_end)]=np.hstack([image_flat,trigger])              \n",
    "\n",
    "            \n",
    "            t+=1\n",
    "            bar.update(t)\n",
    "        bar.finish()\n",
    "    return CNN_data,CNN_images,CNN_labels,CNN_im_shape,CNN_lb_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_data(input_dic, output_dic,mode=\"OUT\"):\n",
    "    \n",
    "    #Получает на вход два словаря  с инпутами  и аутпутами, соответсвующими одинаковым ключам\n",
    "    #В словарях для каждого ключа сохранена матрицам определенных размеров\n",
    "    #В этих матрицах надо инвертировать порядок измерений (для соответсвия требованиям СNTK), \n",
    "    # а потом уплощить и сохранить в новый словарь\n",
    "    \n",
    "    print (\"Started transposing data\")\n",
    "    data_trans_flat={}\n",
    "    for key in input_dic:\n",
    "        input_trans_flat=np.transpose(input_dic[key]).flatten()\n",
    "        output_trans_flat=np.transpose(output_dic[key]).flatten() if mode==\"OUT\" else 0\n",
    "        data_trans_flat[key]=np.hstack((input_trans_flat,output_trans_flat))\n",
    "    return data_trans_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data files into a format compatible with CNTK text reader (формат CTF)\n",
    "# Источник - Майкрософт +модификации\n",
    "def savetxt_chP_op4_fin(filename, data_dic, label_len, norm=False):\n",
    "    dir = os.path.dirname(filename)\n",
    "\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    if not os.path.isfile(filename):\n",
    "        print(\"Saving\", filename )\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            bar = pbar.ProgressBar(max_value=len(data_dic)).start() # Создаём новый progress bar\n",
    "            t=0\n",
    "            for key in data_dic:\n",
    "                row=data_dic[key]\n",
    "                row_str = row.astype(str)\n",
    "                label_str = ' '.join(row_str[-label_len:])#последние label_len элемента в строке -метка\n",
    "                if norm: \n",
    "                    #print (' '.join((row[:-label_len])))\n",
    "                    feature_str= ' '.join((row[:-label_len]//255).astype(str))#все: кроме последних с разделителем \" \"\n",
    "                else:feature_str = ' '.join(row_str[:-label_len])#все: кроме последних с разделителем \" \" \n",
    "                key_str=' '.join(key)\n",
    "                f.write('|keys {}|labels {}|features {}\\n'.format(key_str, label_str, feature_str))\n",
    "                t+=1\n",
    "                bar.update(t)\n",
    "            bar.finish()\n",
    "    else:\n",
    "        print(\"File already exists\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask (input_set, source, stockname, date1 ,date2=None):\n",
    "    \n",
    "    mask_source=input_set['Source']==source\n",
    "    mask_stock=input_set['Stockname']==stockname\n",
    "    if date2==None:\n",
    "        mask_dates=input_set['Date']==date1\n",
    "    else:\n",
    "        mask_date1=input_set['Date']>date1\n",
    "        mask_date2=input_set['Date']<=date2\n",
    "        mask_dates=mask_date1&mask_date2\n",
    "    mask=mask_source&mask_stock&mask_dates\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cnf_matrix(y_true, y_pred,class_names,labels=None):\n",
    "    %matplotlib inline\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = metrics.confusion_matrix(y_true,y_pred,labels)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    cm=plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                          title='Confusion matrix, without normalization')\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    plt.figure()\n",
    "    cm_norm=plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                          title='Normalized confusion matrix')\n",
    "\n",
    "    plt.show()\n",
    "    return cm, cm_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Запуск алгоритма симуляции торгов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#определяем источник данных\n",
    "#path=os.path.join('C:/', 'Users', 'elzolotareva','Desktop','Alfa-Capital-VM')#виртуальная машина\n",
    "path=os.path.abspath(os.curdir)#текущая папка\n",
    "# Определяем исходные данные для моделей\n",
    "saveas=\"121017cnn_MOEX\" \n",
    "# определяем, надо ли логарифмировать данные для модели\n",
    "logarithm=True\n",
    "# если да, то прибавлем \"_ln_\" к метке saveas\n",
    "ln=\"_ln_\"if logarithm else \"\"\n",
    "saveas=ln+saveas\n",
    "\n",
    "#Определяем параметры расчета\n",
    "mode=\"IN\"#OUT означает, что в файле есть ground truth и с ней надо сравнить\n",
    "clean=False# True означает, что мы будет работать с файлом, где устранены разногласия экспертов (для размеченных)\n",
    "test_only=True #True означает, что для экономии времени мы размечаем только тестовую выборку\n",
    "data_status=1\n",
    "#3 - есть готовые файлы как в формате CTF, так и CSV\n",
    "#2 - есть подготовленый файл CSV, но нет CTF\n",
    "#1 - есть подготовленные файлы с предыдущих этапов, требующие обработки\n",
    "#0 - сырые данные, их нужно предварительно очистить и подготовить\n",
    "lag=5 #на случай, если попадутся необработанные данные, в которых необходимо будет выполнить move_triggers\n",
    "\n",
    "dpi_ChP=10#Разрешение картинок для моделей, оценивающих наличие и место смены тенденции\n",
    "dpi_TF=10#Разрешение картинок для модели, определяющей тип тенденции\n",
    "\n",
    "ChP_shape=(int(dpi_ChP/10*48),int(dpi_ChP/10*64),3)#Размер матрицы признаков для моделей ChP\n",
    "TF_shape=(int(dpi_TF/10*48),int(dpi_TF/10*64),3)#Размер матрицы признаков для модели TF\n",
    "ChP_lb = 1#размерность пространства меток для моделей ChP\n",
    "TF_lb = 3#размерность пространства меток для модели TF\n",
    "\n",
    "n_days=75\n",
    "#n_days=25 #Размер среза данных для оценки.\n",
    "#Предпочительно, чтобы он совпадал с показателем n_days, \n",
    "#по которому обучены модели ChP. Иначе результат не предсказуем.\n",
    "skip=5 #Шаг - влияет на объем данных и скорость работы. \n",
    "\n",
    "# Если модели обучены на нормированных данных, то и давать им надо нормированные\n",
    "ChP_norm=True \n",
    "TF_norm=True\n",
    "norm_str=\"_norm\" if ChP_norm else \"\"\n",
    "\n",
    "ChP_с_id=\"6\"# id модели ChP-классификатора для 75 дневок\n",
    "#ChP_с_id=\"4\"# id модели ChP-классификатора для 25 дневок\n",
    "\n",
    "ChP_r_id=\"13\"# id модели ChP-регрессии для 75 дневок\n",
    "#ChP_r_id=\"15\"# id модели ChP-регрессии для 25 дневок\n",
    "\n",
    "TF_id=\"19\"# id модели TF\n",
    "\n",
    "# название файлов, в которых сохранены модели ChP и TF\n",
    "ChP_c_file=\"CNN_ChP_trans_op4_6.model\"\n",
    "#ChP_c_file=\"CNN_ChP_trans_op4_4.model\"\n",
    "\n",
    "ChP_r_file=\"CNN_ChP_trans_op3_13.model\"\n",
    "#ChP_r_file=\"CNN_ChP_trans_op3_15.model\"\n",
    "\n",
    "TF_file=\"CNN_19_trans.model\"\n",
    "\n",
    "    \n",
    "tresh=0.5 #регулируем количество ложных срабатываний\n",
    "\n",
    "d_split=dt.datetime.strptime(\"2014-10-17\",\"%Y-%m-%d\")#для моделей, обученных на всех данных(Барк,Брагин)\n",
    "d_spl=\"_up_to_\"+str(d_split.day)+\"-\"+ str(d_split.month)+\"-\"+str(d_split.year)\n",
    "\n",
    "if data_status==0: #  указываем источник (папку) и расширение для сырых данных\n",
    "    raw_data_source = [\"IN\"] #папка(папки), в которой лежат сырые данные\n",
    "    #extention=\"csv\"#расширение, в котором сохранены исходные данные  - csv или xlsx\n",
    "    extention=\"xlsx\"#расширение, в котором сохранены исходные данные  - csv или xlsx\n",
    "    saveas=\"_\".join(raw_data_source)\n",
    "    eval_file_name=\"Test_ChPop4_trans_keys\"+str(dpi_ChP)+\"dpi_\"+str(n_days)+\"-\"+str(skip)+\"d_\"+saveas+ln+norm_str+\".txt\"\n",
    "elif data_status==1:    \n",
    "#указываем название файлов, где лежат предварительно обработанные данные\n",
    "    NewAllChangePoints_file= \"NewAllChangePointsMOEX_ln_.csv\" #файл с массивом NewAllChangePoints для MOEX\n",
    "    CleanPoints_file=\"NewAllChangePointsMOEX_ln_.csv\" #файл с массивом NewAllChangePoints для MOEX\n",
    "    eval_file_name=\"Test_ChPop4_trans_keys\"+str(dpi_ChP)+\"dpi_\"+str(n_days)+\"-\"+str(skip)+\"d_\"+saveas+ln+norm_str+\".txt\"\n",
    "    #NewAllChangePoints_file= \"NewAllChangePoints_ln_121017v2BB.csv\" #файл с массивом NewAllChangePoints\n",
    "    #CleanPoints_file=\"CleanPoints_ln_121017v2BB.csv\"\n",
    "else:\n",
    "    CNN_Trends_file=\"CNN_Trends_Triggers\"+saveas+\".csv\"\n",
    "    CleanCNN_Trends_file=\"CNN_Trends_Triggers_clean\" +saveas+\".csv\"\n",
    "    if data_status==3:\n",
    "        #указываем названия файлов в формате CTF, пригодном для CNTK\n",
    "        #test_file_name =\"Test_ChPop4_trans_keys\"+str(dpi_ChP)+\"dpi_\"+str(n_days)+\"-\"+str(skip)+\"d_\"+\"clean\"+norm_str+\".txt\"\n",
    "        eval_file_name =\"Test_ChPop4_trans_keys10dpi_75-5d_clean_norm_v2.txt\"\n",
    "        #eval_file_name =\"Test_ChPop4_trans_keys10dpi_25-5d_clean_norm_v2.txt\"\n",
    "# название пайплайна\n",
    "cl=\"clean\" if clean else \"\"\n",
    "pipe_name =mode+\"_ChPс\"+ChP_с_id+\"_ChPr\"+ChP_r_id+\"_TF\"+TF_id +ln+cl+d_spl+\"_tresh=\"+str(tresh)+saveas\n",
    "pipe_file_name=os.path.join(path, \"CNNpipe_\"+pipe_name+\"_\")#имя, под которым будут сохраняться результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elzolotareva\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\elzolotareva\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "N/A% (0 of 137) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started processing images from  1  sources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (137 of 137) |#######################| Elapsed Time: 0:29:17 Time: 0:29:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started transposing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (5 of 20947) |                       | Elapsed Time: 0:00:00 ETA:  0:07:03"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving C:\\Users\\elzolotareva\\Desktop\\Alfa-Capital-VM\\Test_ChPop4_trans_keys10dpi_75-5d__ln_121017cnn_MOEX_ln__norm.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (20947 of 20947) |###################| Elapsed Time: 0:07:24 Time: 0:07:24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 37min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "CNN_relevant_cols=[]\n",
    "indep_cols=[]\n",
    "key=[\"Source\",\"Stockname\",\"Date\"]\n",
    "CNN_key=[\"Source\",\"Stockname\",\"Date_srt\", \"Date_end\"]\n",
    "num_cols=[\"Open\",\"High\",\"Low\",\"Close\", \"Volume\"]\n",
    "indep_cols.extend(key)\n",
    "indep_cols.extend(num_cols)\n",
    "label_cols=[\"NewTypeBool_sign\",'NewTrigger']\n",
    "CNN_relevant_cols.extend(indep_cols)\n",
    "if mode==\"OUT\":CNN_relevant_cols.extend(label_cols)\n",
    "\n",
    "CTF_columns=[\"keys\",\"features\"]\n",
    "if mode==\"OUT\":CTF_columns.extend([\"labels\"])\n",
    "    \n",
    "#Загружаем данные\n",
    "if data_status==0:\n",
    "    #Обрабатываем сырые данные\n",
    "    new_data =pd.DataFrame()\n",
    "    for source in raw_data_source:\n",
    "        new_data_tmp=trd.join_files(os.path.join(path, source),\"\",extention=extention, mode=mode,make_ln=logarithm)\n",
    "        new_data=pd.concat([new_data,new_data_tmp])\n",
    "    new_data=new_data.reset_index(drop=True)\n",
    "    \n",
    "    #Округляем, чтобы исключить технические ошибки\n",
    "    new_data[num_cols]= new_data[num_cols].round(10)\n",
    "    # список колонок, которые содержат независимые (от эксперта) сведения о состоянии рынка\n",
    "    \n",
    "    if trd.check_for_unique_keys(key,indep_cols,new_data):\n",
    "        CNN_trends = new_data\n",
    "        if mode==\"OUT\": \n",
    "            #заменяем \"серые зоны\" на флэт\n",
    "            CNN_Trends=trd.UnknownToFlat(CNN_Trends)\n",
    "            #определеяем точки перелома тендеции\n",
    "            CNN_Trends=trd.fill_trigger_by_idselect(CNN_trends)\n",
    "            CNN_Trends=trd.add_direction(CNN_Trends,\"IDselect_tmp\", \"NewTypeBool\")\n",
    "            CNN_Trends=trd.move_triggers(CNN_Trends,lag,False)\n",
    "            if clean:\n",
    "                CleanCNN_Trends=trd.clean_changepoints (CNN_Trends,type_col=\"NewTypeBool_sign\")\n",
    "                CleanCNN_Trends=trd.fill_idselect(CleanCNN_Trends,\"IDselect\",\"NewTrigger\")\n",
    "                CleanCNN_Trends=trd.move_triggers (CleanCNN_Trends,lag,True)\n",
    "                CleanCNN_Trends.to_csv(os.path.join(path,\"CNN_Trends_Triggers_clean\" +saveas+\".csv\"),sep = ',', index = False,header=1,encoding='cp1251')\n",
    "        CNN_Trends.to_csv(os.path.join(path,\"CNN_Trends_Triggers\" +saveas+\".csv\"),sep = ',', index = False,header=1,encoding='cp1251')   \n",
    "elif data_status==1:\n",
    "    #Загружаем подготовленные исходные данные из файла \n",
    "    NewAllChangePoints=pd.read_csv(os.path.join(path, NewAllChangePoints_file), header=0, sep=',',encoding='cp1251')\n",
    "    NewAllChangePoints=restore_prices_volumes(NewAllChangePoints, logarithm)\n",
    "    cols_to_copy=['File_id']\n",
    "    if mode==\"OUT\":cols_to_copy.extend([\"IDselect_tmp\", \"Username\"])\n",
    "    cols_to_copy.extend(CNN_relevant_cols)\n",
    "    CNN_Trends=NewAllChangePoints[cols_to_copy]\n",
    "    CNN_Trends.to_csv(os.path.join(path,\"CNN_Trends_Triggers\" +saveas+\".csv\"),sep = ',', index = False,header=1,encoding='cp1251')   \n",
    "    if clean:\n",
    "        CleanPoints= pd.read_csv(os.path.join(path, CleanPoints_file), header=0, sep=',',encoding='cp1251')\n",
    "        CleanPoints= restore_prices_volumes(CleanPoints, logarithm)\n",
    "        cols_to_copy=[]\n",
    "        if mode==\"OUT\":cols_to_copy.extend([\"IDselect\"])\n",
    "        cols_to_copy.extend(CNN_relevant_cols)\n",
    "        CleanCNN_Trends=CleanPoints[cols_to_copy]\n",
    "        CleanCNN_Trends.to_csv(os.path.join(path,\"CNN_Trends_Triggers_clean\" +saveas+\".csv\"),sep = ',', index = False,header=1,encoding='cp1251')\n",
    "else:\n",
    "    CNN_Trends=pd.read_csv(os.path.join(path, CNN_Trends_file), header=0, sep=',',encoding='cp1251')\n",
    "    if clean:CleanCNN_Trends=pd.read_csv(os.path.join(path, CleanCNN_Trends_file), header=0, sep=',',encoding='cp1251')\n",
    "        \n",
    "#приводим даты к формату даты, чтобы можно было сортировать по дате\n",
    "CNN_Trends[\"Date\"]=pd.to_datetime(CNN_Trends[\"Date\"], format = \"%Y/%m/%d\")\n",
    "CNN_Trends.sort_values([\"File_id\",\"Date\"], ascending=True, inplace=True)\n",
    "if clean:\n",
    "    CleanCNN_Trends[\"Date\"]=pd.to_datetime(CleanCNN_Trends[\"Date\"], format = \"%Y/%m/%d\")\n",
    "    CleanCNN_Trends.sort_values(['Source', 'Stockname',\"Date\"], ascending=True, inplace=True)\n",
    "\n",
    "if data_status<3:\n",
    "    #далее можно работать с исходным файлом или с очищенным (с усредненными оценками)\n",
    "    CTF_base=CleanCNN_Trends if clean else CNN_Trends\n",
    "    eval_set=CTF_base[CTF_base[\"Date\"]>d_split] if test_only else CTF_base\n",
    "    CNN_process_and_save(eval_set,\"NewTrigger\",dpi_ChP,n_days,skip,eval_file_name,ChP_norm,mode)\n",
    "    \n",
    "CTF_data, eval_size =ctf_parser(os.path.join(path,eval_file_name), CTF_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 и 2. Шаг первый и второй:\n",
    "\n",
    "1. определяем, есть ли в срезе данных точка смены тенденции (модель ChP_c), и если да, то где она (модель СhP_r)\n",
    "\n",
    "2. определяем текущую тенденцию в выделенном окне (модель TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#далее можно работать с исходным файлом или с очищенным (с усредненными оценками)\n",
    "AllChangePoints=CleanCNN_Trends if clean else CNN_Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232489 232489\n",
      "The keys are unique. You may proceed\n",
      "232489 232489\n",
      "The keys are unique. You may proceed\n",
      "Wall time: 755 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Проверяем ключ на уникальность\n",
    "trd.check_for_unique_keys(key,indep_cols,AllChangePoints)\n",
    "#Проверяем ключ на уникальность\n",
    "trd.check_for_unique_keys(key,indep_cols,CNN_Trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(232489, 9) original\n",
      "(232489, 9) relevant, no duplicates\n",
      "Wall time: 289 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Удаляем строки, в которых есть дубликаты\n",
    "print (AllChangePoints.shape, \"original\")\n",
    "SubAllChangePoints=AllChangePoints.drop_duplicates(subset=indep_cols)#если загружен clean, то дубликатов не должно быть\n",
    "print (SubAllChangePoints.shape, \"relevant, no duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фиксируем параметры, отвечающие за \"случайность\" для возможности воспроизведения результата\n",
    "np.random.seed(0)\n",
    "C.cntk_py.set_fixed_random_seed(1)\n",
    "C.cntk_py.force_deterministic_algorithms()\n",
    "\n",
    "# Определяем размерность данных\n",
    "ChP_dim_model = ChP_shape [::-1]  # images are 384 (image width) x 288 (image height) with 3 channel of color (RGB) \n",
    "TF_dim_model = TF_shape [::-1]\n",
    "#ChP_dim = int(np.prod(ChP_shape)) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_ChP_c = C.Function.load(os.path.join(path, ChP_c_file))\n",
    "saved_ChP_r = C.Function.load(os.path.join(path, ChP_r_file))\n",
    "saved_TF = C.Function.load(os.path.join(path, TF_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20947, 4) original\n",
      "(20947, 4) no duplicates\n"
     ]
    }
   ],
   "source": [
    "ChP_keys=np.array([CTF_data[\"keys\",i] for i in range(eval_size)])\n",
    "#Важно,чтобы индекс фрейма ChP_keys соответсвовал номеру записи в словаре с features\n",
    "ChP_keys=pd.DataFrame(ChP_keys,columns=CNN_key,index=range(eval_size))\n",
    "ChP_keys[\"Date_srt\"]=pd.to_datetime(ChP_keys[\"Date_srt\"], format = \"%Y-%m-%d\")\n",
    "ChP_keys[\"Date_end\"]=pd.to_datetime(ChP_keys[\"Date_end\"], format = \"%Y-%m-%d\")\n",
    "print (ChP_keys.shape, \"original\")\n",
    "ChP_keys.drop_duplicates(inplace=True)#на тот случай, если CTF-датасет содержит метки разных экспертов (дублирующиеся features)\n",
    "print (ChP_keys.shape, \"no duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1% (297 of 20947) |                     | Elapsed Time: 0:01:07 ETA:  1:18:42"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_error 295 MOEX AQUA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10% (2294 of 20947) |##                  | Elapsed Time: 0:08:44 ETA:  1:12:21"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_error 2292 MOEX ROSB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26% (5654 of 20947) |#####               | Elapsed Time: 0:21:49 ETA:  1:00:02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_error 5652 MOEX VSYD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37% (7794 of 20947) |#######             | Elapsed Time: 0:30:13 ETA:  0:52:21"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_error 7792 MOEX BISV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45% (9635 of 20947) |#########           | Elapsed Time: 0:37:34 ETA:  0:45:22"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_error 9634 MOEX EELT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49% (10436 of 20947) |#########          | Elapsed Time: 0:40:45 ETA:  0:41:54"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_error 10434 MOEX GRNT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63% (13326 of 20947) |############       | Elapsed Time: 0:52:23 ETA:  0:30:41"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_error 13324 MOEX KTSB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64% (13483 of 20947) |############       | Elapsed Time: 0:53:02 ETA:  0:30:19"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_error 13481 MOEX KUZB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78% (16507 of 20947) |##############     | Elapsed Time: 1:05:23 ETA:  0:18:20"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_error 16505 MOEX MISBP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90% (18923 of 20947) |#################  | Elapsed Time: 1:15:25 ETA:  0:08:23"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_error 18921 MOEX NKNC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90% (19058 of 20947) |#################  | Elapsed Time: 1:15:58 ETA:  0:07:51"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_error 19056 MOEX NKSH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (20947 of 20947) |###################| Elapsed Time: 1:24:01 Time: 1:24:01\n"
     ]
    }
   ],
   "source": [
    "TF_predictions=pd.DataFrame(columns=[\"Source\", \"Stockname\",\"Win_srt\",\"Win_end\",\"Valid_till\", \"ScoredNewTypeBool\"])\n",
    "ChP_predictions=pd.DataFrame(columns=[\"Source\", \"Stockname\",\"Date_srt\",\"Date_end\",\"Trigger_score\", \"Trigger_pred\",\"Position_pred\"])\n",
    "\n",
    "mappedSubAllChangePoints=SubAllChangePoints.copy()\n",
    "bar = pbar.ProgressBar(max_value=ChP_keys.shape[0]).start() # Создаём новый progress bar\n",
    "t=0\n",
    "srange=list(ChP_keys['Source'].drop_duplicates())\n",
    "for s in srange:#для каждого источника \n",
    "    stock_set=ChP_keys[ChP_keys['Source']==s]#выбрали источник\n",
    "    frange=list(stock_set['Stockname'].drop_duplicates())#составляем список акций в этом источнике\n",
    "    for f in frange:# перебираем все файлы(акции)\n",
    "        TrendNew=stock_set[stock_set[\"Stockname\"]==f]\n",
    "        win_srt=ChP_keys.loc[TrendNew.index[0], \"Date_srt\"]\n",
    "        #print(s,f,TrendNew.index)\n",
    "        for i in TrendNew.index:\n",
    "            win_end=ChP_keys.loc[i, \"Date_end\"]\n",
    "            if i!=TrendNew.index[0]:\n",
    "                TF_predictions.loc[i_last,\"Valid_till\"]=win_end\n",
    "                d1=TF_predictions.loc[i_last,\"Win_end\"]\n",
    "                d2=TF_predictions.loc[i_last,\"Valid_till\"]\n",
    "                mask_valid=create_mask(mappedSubAllChangePoints,s,f,d1,d2)\n",
    "                idx=mappedSubAllChangePoints[mask_valid].index\n",
    "                mappedSubAllChangePoints.loc[idx, \"ScoredNewTypeBool\"]=TF_predictions.loc[i_last,\"ScoredNewTypeBool\"]\n",
    "                \n",
    "            ChP_x=np.reshape(CTF_data[\"features\",i],ChP_dim_model).astype(np.float32)\n",
    "            ChP_c_score=np.asscalar(saved_ChP_c.eval(ChP_x))\n",
    "            ChP_c_pred=1 if ChP_c_score>=tresh else 0\n",
    "            ChP_r_pred=\"\"\n",
    "            if ChP_c_pred==1: \n",
    "                ChP_r_pred=np.clip(np.asscalar(saved_ChP_r.eval(ChP_x)),0,1)\n",
    "                TrendNew_len=ChP_keys.loc[i, \"Date_end\"]-ChP_keys.loc[i, \"Date_srt\"]\n",
    "                #print(TrendNew_len,ChP_r_pred,ChP_keys.loc[i, \"Date_srt\"])\n",
    "                win_srt=ChP_keys.loc[i, \"Date_srt\"]+ChP_r_pred*TrendNew_len\n",
    "                win_srt=pd.to_datetime(win_srt.date(), format = \"%Y-%m-%d\")\n",
    "                #print (win_srt)\n",
    "            ChP_predictions.loc[i]=[s,f,ChP_keys.loc[i,\"Date_srt\"],win_end,ChP_c_score,ChP_c_pred,ChP_r_pred]\n",
    "            \n",
    "            mask_win=create_mask(SubAllChangePoints,s,f,win_srt-dt.timedelta(minutes=5),win_end)\n",
    "            TaggedWin=SubAllChangePoints[mask_win].copy()\n",
    "            #win_srt=TaggedWin.loc[TaggedWin.index[0],\"Date\"]\n",
    "            TF_x =get_picture(TaggedWin,dpi_TF)[2]\n",
    "            TF_x =np.transpose(TF_x).flatten()\n",
    "            TF_x =np.reshape(TF_x,TF_dim_model).astype(np.float32) \n",
    "            #print (np.argmax(saved_TF.eval(TF_x)))\n",
    "            TF_pred=[0,1,-1] [np.argmax(saved_TF.eval(TF_x))]\n",
    "            TF_predictions.loc[i]=[s,f, win_srt, win_end,win_end,TF_pred]\n",
    "            i_last=i\n",
    "            t+=1\n",
    "            bar.update(t)\n",
    "        d1=TF_predictions.loc[i_last,\"Win_end\"]\n",
    "        d2=dt.datetime.now().date()\n",
    "        mask_valid=create_mask(mappedSubAllChangePoints,s,f,d1,d2)\n",
    "        idx=mappedSubAllChangePoints[mask_valid].index\n",
    "        mappedSubAllChangePoints.loc[idx, \"ScoredNewTypeBool\"]=TF_predictions.loc[i_last,\"ScoredNewTypeBool\"]\n",
    "        try:\n",
    "            TF_predictions.loc[i_last,\"Valid_till\"]=mappedSubAllChangePoints.loc[idx[-1], \"Date\"]\n",
    "        except IndexError:\n",
    "            print (\"index_error\",i_last, s, f)\n",
    "            continue\n",
    "bar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode ==\"OUT\":\n",
    "    ChP_labels=np.array([CTF_data[\"labels\",i] for i in range(eval_size)])\n",
    "    ChP_labels=pd.DataFrame(ChP_labels,columns=[\"Trigger_label\"],dtype=np.float32,index=range(eval_size))\n",
    "    ChP_labels=pd.merge(ChP_keys,ChP_labels, left_index=True,right_index=True)\n",
    "    ChP_predictions=pd.merge(ChP_predictions,ChP_labels,on=CNN_key,suffixes=(\"\",\"_copy\"))\n",
    "    ChP_names=['NoChange', 'Change']\n",
    "    ChP_rep_f,ChP_acc_f,ChP_auc_f=trd.validate_model(ChP_names,ChP_predictions[\"Trigger_label\"],ChP_predictions[\"Trigger_score\"],tresh)\n",
    "    show_cnf_matrix(ChP_predictions[\"Trigger_label\"],ChP_predictions[\"Trigger_pred\"].astype(np.float32),ChP_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChP_predictions.to_csv(os.path.join(path,pipe_file_name+\"ChP_predictions\" +saveas+\".csv\"),sep = ',', index = False,header=1,encoding='cp1251')\n",
    "TF_predictions.to_csv(os.path.join(path,pipe_file_name+\"TF_predictions\" +saveas+\".csv\"),sep = ',', index = False,header=1,encoding='cp1251')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappedSubAllChangePoints['ScoredNewTrigger_tmp']=0\n",
    "idx=mappedSubAllChangePoints[mappedSubAllChangePoints[\"Date\"].isin(list(TF_predictions[\"Win_srt\"]))].index\n",
    "mappedSubAllChangePoints.loc[idx,'ScoredNewTrigger_tmp']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Шаг третий. Финальная обработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (136 of 136) |#######################| Elapsed Time: 0:00:02 Time: 0:00:02\n"
     ]
    }
   ],
   "source": [
    "#Создаем финальную колонку ScoredNewTrigger\n",
    "mappedSubAllChangePoints_short=mappedSubAllChangePoints[[\"Source\",\"Stockname\",\"Date\",\"ScoredNewTypeBool\"]].dropna()\n",
    "mappedSubAllChangePoints_short=trd.fill_trigger_by_type(mappedSubAllChangePoints_short,\n",
    "                                              trigger_col=\"ScoredNewTrigger\", type_col=\"ScoredNewTypeBool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (136 of 136) |#######################| Elapsed Time: 0:00:03 Time: 0:00:03\n"
     ]
    }
   ],
   "source": [
    "#Создаем финальную колонку ScoredNewIDselect\n",
    "filledIDselect=trd.fill_idselect(mappedSubAllChangePoints_short,\n",
    "                             IDSelect_col=\"ScoredNewIDselect\",trigger_col=\"ScoredNewTrigger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104422 104422\n",
      "The keys are unique. You may proceed\n",
      "Wall time: 257 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Соединяем с остальным массивом\n",
    "key=[\"Stockname\",\"Date\",\"Source\"]\n",
    "if trd.check_for_unique_keys(key,filledIDselect.columns,filledIDselect):\n",
    "    mappedSubAllChangePoints=pd.merge(mappedSubAllChangePoints,filledIDselect, copy=False, how=\"left\", on=key,suffixes=(\"\",\"_copy\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#В массиве CleanPoints на самом деле лежат усредненные данные.Переименовываем колонки\n",
    "if clean==True: \n",
    "    mappedSubAllChangePoints=mappedSubAllChangePoints.rename(columns={\"NewTypeBool_sign\": 'NewTypeBool_Avg'})\n",
    "    mappedSubAllChangePoints=mappedSubAllChangePoints.rename(columns={\"NewTrigger\": 'NewTrigger_Avg'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ScoredNewTypeBool_copy']\n"
     ]
    }
   ],
   "source": [
    "#В ходе операций merge часть колонок могла скопироваться. Их надо удалить\n",
    "cols_to_del=[]\n",
    "for c in list(mappedSubAllChangePoints.columns):\n",
    "            if (c.find(\"copy\")>-1): cols_to_del.append(c)#добавляем в черный список\n",
    "print (cols_to_del)\n",
    "#Удаляем\n",
    "mappedSubAllChangePoints.drop(cols_to_del,axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 328 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Объединяем с исходным массивом\n",
    "key=[\"Date\",\"Stockname\",\"Source\"]\n",
    "mappedNewAllChangePoints=pd.merge(CNN_Trends,mappedSubAllChangePoints.dropna(),\n",
    "                                  how=\"left\", on=key, suffixes=(\"\",\"_copy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['File_id_copy', 'Open_copy', 'High_copy', 'Low_copy', 'Close_copy', 'Volume_copy']\n"
     ]
    }
   ],
   "source": [
    "#В ходе операций merge часть колонок могла скопироваться. Их надо удалить\n",
    "cols_to_del=[]\n",
    "for c in list(mappedNewAllChangePoints.columns):\n",
    "            if (c.find(\"copy\")>-1): cols_to_del.append(c)#добавляем в черный список\n",
    "print (cols_to_del)\n",
    "#Удаляем\n",
    "mappedNewAllChangePoints.drop(cols_to_del,axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#сохраняем новые размеченные массивы для истории \n",
    "mappedNewAllChangePoints.to_csv(os.path.join(path, pipe_file_name+\"_results.csv\"), header=1, sep=',',encoding='cp1251')\n",
    "mappedSubAllChangePoints.to_csv(os.path.join(path, pipe_file_name+\"_sub_results.csv\"), header=1, sep=',',encoding='cp1251')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Оценка качества итоговой разметки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elzolotareva\\Desktop\\Alfa-Capital-VM\\trend_functions.py:688: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  scored_set[\"Close_orig\"] = np.exp(scored_set[\"Close\"])\n",
      "100% (136 of 136) |#######################| Elapsed Time: 0:00:05 Time: 0:00:05\n"
     ]
    }
   ],
   "source": [
    "#Считаем прибыли-убытки по предсказаниям модели\n",
    "QuickStats=pd.DataFrame(columns=[\"Model\",\"numStocks\",\"Profit\",\"Days_in\",\"Times_in\",\"dayProfit\"])\n",
    "PL_rep_model=trd.PL_report(mappedSubAllChangePoints.dropna(),\n",
    "                 IDselectCol=\"ScoredNewIDselect\",TypeBoolCol=\"ScoredNewTypeBool\",logarithm=logarithm)\n",
    "PL_rep_model.to_excel(os.path.join(path,pipe_file_name+\"_PL_ReportLngSht_model.xlsx\"))\n",
    "QuickStats.loc[0]=[\"Model\", PL_rep_model.shape[0], PL_rep_model[\"Profit\"].sum(),PL_rep_model[\"Days_in\"].sum(),\n",
    "                   PL_rep_model[\"Times_in\"].sum(), PL_rep_model[\"Profit\"].sum()/PL_rep_model[\"Days_in\"].sum()]\n",
    "#print (PL_rep_model[\"Profit\"].sum(), PL_rep_model[\"Profit_lng\"].sum(), PL_rep_model[\"Profit_sht\"].sum(),\"model\")\n",
    "#для сравнения считаем PL по усредненным оценкам экспертов\n",
    "if mode==\"OUT\" and clean==True:\n",
    "    cleanIDselect=trd.fill_idselect(mappedSubAllChangePoints.dropna(),IDSelect_col=\"NewIDselect_Avg\",trigger_col=\"NewTrigger_Avg\")\n",
    "    PL_rep_gt=trd.PL_report(cleanIDselect.dropna(),\n",
    "            IDselectCol=\"NewIDselect_Avg\",TypeBoolCol=\"NewTypeBool_Avg\",logarithm=logarithm)\n",
    "    PL_rep_gt.to_excel(os.path.join(path, pipe_file_name+\"_PL_ReportLngSht_ground_truth.xlsx\"))\n",
    "    #print (PL_rep_gt[\"Profit\"].sum(), PL_rep_gt[\"Profit_lng\"].sum(), PL_rep_gt[\"Profit_sht\"].sum(),\"ground_truth\")\n",
    "    QuickStats.loc[1]=[\"Ground_truth\", PL_rep_gt.shape[0], PL_rep_gt[\"Profit\"].sum(),PL_rep_gt[\"Days_in\"].sum(),\n",
    "                   PL_rep_gt[\"Times_in\"].sum(), PL_rep_gt[\"Profit\"].sum()/PL_rep_gt[\"Days_in\"].sum()]\n",
    "QuickStats.to_excel(os.path.join(path,pipe_file_name+\"_QuickStats.xlsx\"))\n",
    "#QuickStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>numStocks</th>\n",
       "      <th>Profit</th>\n",
       "      <th>Days_in</th>\n",
       "      <th>Times_in</th>\n",
       "      <th>dayProfit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model</td>\n",
       "      <td>136</td>\n",
       "      <td>607.12578</td>\n",
       "      <td>16311</td>\n",
       "      <td>869</td>\n",
       "      <td>0.037222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model numStocks     Profit Days_in Times_in  dayProfit\n",
       "0  Model       136  607.12578   16311      869   0.037222"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QuickStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Оцениваем качество разметки по файлу mappedNewAllChangePoints (возможны разногласия экспертов)\n",
    "if mode ==\"OUT\":\n",
    "    pipe_names = ['Flat', 'Trend']\n",
    "    y_pipe=abs(mappedNewAllChangePoints.dropna()[\"NewTypeBool_sign\"])\n",
    "    #print (y_pipe.shape)\n",
    "    y_pipe_pred=abs(mappedNewAllChangePoints.dropna()[\"ScoredNewTypeBool\"])\n",
    "    #print (y_pipe_pred.shape)\n",
    "    pipe_rep_f,pipe_acc_f,pipe_auc_f=trd.validate_model(pipe_names,y_pipe,y_pipe_pred,0.5)\n",
    "    if test_only==False:\n",
    "\n",
    "        pipe_train=mappedNewAllChangePoints[mappedNewAllChangePoints[\"Date\"]<=d_split]\n",
    "        pipe_test=mappedNewAllChangePoints[mappedNewAllChangePoints[\"Date\"]>d_split]\n",
    "        \n",
    "        pipe_train.dropna(inplace=True)\n",
    "        pipe_test.dropna(inplace=True)\n",
    "        y_pipe_train=abs(pipe_train[\"NewTypeBool_sign\"])\n",
    "        y_pipe_test=abs(pipe_test[\"NewTypeBool_sign\"])\n",
    "        y_pipe_train_pred=abs(pipe_train[\"ScoredNewTypeBool\"])\n",
    "        y_pipe_test_pred=abs(pipe_test[\"ScoredNewTypeBool\"])\n",
    "        pipe_rep_t,pipe_acc_t,pipe_auc_t=trd.validate_model(pipe_names,y_pipe_train,y_pipe_train_pred,0.5)\n",
    "        pipe_rep_v,pipe_acc_v,pipe_auc_v=trd.validate_model(pipe_names,y_pipe_test,y_pipe_test_pred,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Оцениваем качество разметки по файлу mappedSubAllChangePoints (если в него залито Clean, то сравниваются усредненные эксперты)\n",
    "if (mode ==\"OUT\") and (clean==True):\n",
    "    pipe_names = ['Flat', 'Trend']\n",
    "    y_pipe_cl=abs(mappedSubAllChangePoints.dropna()[\"NewTypeBool_Avg\"])\n",
    "    y_pipe_pred_cl=abs(mappedSubAllChangePoints.dropna()[\"ScoredNewTypeBool\"])\n",
    "    pipe_rep_f_cl,pipe_acc_f_cl,pipe_auc_f_cl=trd.validate_model(pipe_names,y_pipe_cl,y_pipe_pred_cl,0.5)\n",
    "    if test_only==False:\n",
    "        pipe_train_cl=mappedSubAllChangePoints[mappedSubAllChangePoints[\"Date\"]<=d_split]\n",
    "        pipe_test_cl=mappedSubAllChangePoints[mappedSubAllChangePoints[\"Date\"]>d_split]\n",
    "        \n",
    "        pipe_train_cl.dropna(inplace=True)\n",
    "        pipe_test_cl.dropna(inplace=True)\n",
    "        y_pipe_train_cl=abs(pipe_train_cl[\"NewTypeBool_Avg\"])\n",
    "        y_pipe_test_cl=abs(pipe_test_cl[\"NewTypeBool_Avg\"])\n",
    "        y_pipe_train_pred_cl=abs(pipe_train_cl[\"ScoredNewTypeBool\"])\n",
    "        y_pipe_test_pred_cl=abs(pipe_test_cl[\"ScoredNewTypeBool\"])\n",
    "        pipe_rep_t_cl,pipe_acc_t_cl,pipe_auc_t_cl=trd.validate_model(pipe_names,y_pipe_train_cl,y_pipe_train_pred_cl,0.5)\n",
    "        pipe_rep_v_cl,pipe_acc_v_cl,pipe_auc_v_cl=trd.validate_model(pipe_names,y_pipe_test_cl,y_pipe_test_pred_cl,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mappedSubAllChangePoints.dropna()[\"NewTypeBool_Avg\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104422,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mappedSubAllChangePoints.dropna()[\"ScoredNewTypeBool\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Оцениваем качество разметки по файлу mappedSubAllChangePoints (если в него залито Clean, то сравниваются усредненные эксперты)\n",
    "#Для усредненных данных сохраняем еще confusion matrix\n",
    "if (mode ==\"OUT\") and (clean==True):\n",
    "    TF_names = ['Flat', 'Up',\"Down\"]\n",
    "    TF_labels=[0,1,-1]\n",
    "    y_TF_cl=mappedSubAllChangePoints.dropna()[\"NewTypeBool_Avg\"]\n",
    "    y_TF_pred_cl=mappedSubAllChangePoints.dropna()[\"ScoredNewTypeBool\"]\n",
    "    cm, cm_norm=show_cnf_matrix(y_TF_cl, y_TF_pred_cl,TF_names,TF_labels)\n",
    "    \n",
    "    if test_only==False:\n",
    "        TF_train_cl=mappedSubAllChangePoints[mappedSubAllChangePoints[\"Date\"]<=d_split]\n",
    "        TF_test_cl=mappedSubAllChangePoints[mappedSubAllChangePoints[\"Date\"]>d_split]\n",
    "        \n",
    "        TF_train_cl.dropna(inplace=True)\n",
    "        TF_test_cl.dropna(inplace=True)\n",
    "        y_TF_train_cl=TF_train_cl[\"NewTypeBool_Avg\"]\n",
    "        y_TF_test_cl=TF_test_cl[\"NewTypeBool_Avg\"]\n",
    "        y_TF_train_pred_cl=TF_train_cl[\"ScoredNewTypeBool\"]\n",
    "        y_TF_test_pred_cl=TF_test_cl[\"ScoredNewTypeBool\"]\n",
    "        cm_t, cm_t_norm=show_cnf_matrix(y_TF_train_cl, y_TF_train_pred_cl,TF_names,TF_labels)\n",
    "        cm_v, cm_v_norm=show_cnf_matrix(y_TF_test_cl, y_TF_test_pred_cl,TF_names,TF_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#считаем дополнительные метрики по всем данным\n",
    "if mode ==\"OUT\":\n",
    "    key=[\"Source\",\"Stockname\",\"Date\"]\n",
    "    mappedNewAllChangePoints[\"Date\"]=pd.to_datetime(mappedNewAllChangePoints[\"Date\"], format = \"%Y/%m/%d\")\n",
    "    sortedNewAllChangePoints = mappedNewAllChangePoints.sort_values(by=key).dropna()\n",
    "    y_pipe_sort=abs(sortedNewAllChangePoints[\"NewTypeBool_sign\"])\n",
    "    y_pipe_pred_sort=abs(sortedNewAllChangePoints[\"ScoredNewTypeBool\"])\n",
    "    # считаем новую метрику по всем данным\n",
    "    pipe_f=trd.freq(y_pipe_sort.as_matrix().flatten(), y_pipe_pred_sort.as_matrix().flatten())\n",
    "    # считаем другую новую метрику\n",
    "    pipe_d= trd.delay(y_pipe_sort,y_pipe_pred_sort)\n",
    "    print (\"freq=\"+str(pipe_f))\n",
    "    print (\"delay=\"+str(pipe_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#считаем дополнительные метрики по усредненным данным (если залито Clean, то сравниваются усредненные эксперты))\n",
    "if (mode ==\"OUT\") and (clean==True): \n",
    "    key=[\"Source\",\"Stockname\",\"Date\"]\n",
    "    mappedSubAllChangePoints[\"Date\"]=pd.to_datetime(mappedSubAllChangePoints[\"Date\"], format = \"%Y/%m/%d\")\n",
    "    sortedSubAllChangePoints = mappedSubAllChangePoints.sort_values(by=key).dropna()\n",
    "    y_pipe_sort_cl=abs(sortedSubAllChangePoints[\"NewTypeBool_Avg\"])\n",
    "    y_pipe_pred_sort_cl=abs(sortedSubAllChangePoints[\"ScoredNewTypeBool\"])\n",
    "    # считаем новую метрику по усредненным данным\n",
    "    pipe_f_cl=trd.freq(y_pipe_sort_cl.as_matrix().flatten(), y_pipe_pred_cl.as_matrix().flatten())\n",
    "    # считаем другую новую метрику по усредненным данным\n",
    "    pipe_d_cl=trd.delay(y_pipe_sort_cl,y_pipe_pred_sort_cl)\n",
    "    print (\"freq=\"+str(pipe_f_cl))\n",
    "    print (\"delay=\"+str(pipe_d_cl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#сохраняем статистику качества (только для размеченных данных)\n",
    "if mode ==\"OUT\":\n",
    "    Pipe_address= os.path.join(path, pipe_file_name+\"_stats.txt\")\n",
    "    if test_only==False:\n",
    "        trd.save_results(Pipe_address,pipe_rep_t,pipe_acc_t,pipe_auc_t,pipe_rep_v,pipe_acc_v,pipe_auc_v)\n",
    "    Pipeline_stats= open(Pipe_address,\"a\")\n",
    "    text1='\\n'+ \"Full_set\"+'\\n'+ pipe_rep_f +'\\n'+ \"Accuracy: %.2f%%\" % (pipe_acc_f * 100.0)+ '\\n'+\"AUC: %.2f%%\" % (pipe_auc_f * 100.0)\n",
    "    text2='\\n'+ \"Full_set\"+'\\n'+ \"Freq: %.2f%%\" % (pipe_f * 100.0)+ '\\n'+\"Delay: %.2f%%\"% (pipe_d * 100.0) \n",
    "    Pipeline_stats.write(text1+'\\n'+text2)\n",
    "    Pipeline_stats.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#сохраняем статистику качества (только для размеченных данных) - усредненные эксперты\n",
    "#Дополнительно сохраняется confusion_matrix\n",
    "if (mode ==\"OUT\") and (clean==True):\n",
    "    Pipe_address_cl= os.path.join(path, pipe_file_name+\"_sub_stats.txt\")\n",
    "    if test_only==False:\n",
    "        trd.save_results(Pipe_address_cl,pipe_rep_t_cl,pipe_acc_t_cl,pipe_auc_t_cl,str(cm_t), str(cm_t_norm), pipe_rep_v_cl,pipe_acc_v_cl,pipe_auc_v_cl, str(cm_v), str(cm_v_norm))\n",
    "    Pipeline_stats= open(Pipe_address_cl,\"a\")\n",
    "    text1='\\n'+ \"Full_set\"+'\\n'+ pipe_rep_f_cl +'\\n'+ \"Accuracy: %.2f%%\" % (pipe_acc_f_cl * 100.0)+ '\\n'+\"AUC: %.2f%%\" % (pipe_auc_f_cl * 100.0)\n",
    "    text2='\\n'+ \"Full_set\"+'\\n'+ \"Freq: %.2f%%\" % (pipe_f_cl * 100.0)+ '\\n'+\"Delay: %.2f%%\"% (pipe_d_cl* 100.0)\n",
    "    text3='\\n'+ \"Full_set\"+'\\n'+ \"Confusion matrix \"+ str(cm) +'\\n' +\"Normalized confusion matrix \"+ str(cm_norm)\n",
    "    Pipeline_stats.write(text1+'\\n'+text2+ '\\n'+ text3)\n",
    "    Pipeline_stats.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Визуализация\n",
    "\n",
    "Визуализация может работать как в связке с предыдущими пунктами, так и независимо от них (режим регулируется переменной use_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly as py\n",
    "from datetime import timedelta\n",
    "use_current=True##указывает, следует ли использовать настройки, полученные при последнем прогоне Ipython notebook \n",
    "if use_current==False:\n",
    "    #если используются не текущие настройки, то надо указать какие именно\n",
    "    path = os.path.join('/Users', 'KatieHome', 'Desktop', 'FinUniversity','Alpha-project','Alfa-Capital','Logs_moved')\n",
    "    mode=\"OUT\"\n",
    "    logarithm=True\n",
    "    #title=\"Pipeline_OUT_ChP24_TF11_ln_clean_up_to_13-10-2014tresh=0.6_ln_121017\"\n",
    "    #title=\"Pipeline_OUT_ChP25_TF12_ln_cleantresh=0.6_ln_060617\"\n",
    "    #title=\"Pipeline_OUT_ChP25_TF12_ln_clean_up_to_6-11-2014tresh=0.6_ln_121017\"\n",
    "    #title=\"Pipeline_IN_ChP23_TF10_ln__tresh=0.6IN\"\n",
    "    #pipe_file_name=\"Pipeline_OUT_ChP26_TF11_ln_clean_up_to_13-10-2014_tresh=0.85_ln_121017v2\"\n",
    "    pipe_file_name=\"Pipeline_OUT_ChP28_TF12_ln_clean_up_to_6-11-2014_tresh=0.5_ln_121017v2\"\n",
    "    mappedSubAllChangePoints=pd.read_csv(os.path.join(path, pipe_file_name+\"__sub_results.csv\"), header=0, sep=',',encoding='cp1251')\n",
    "    mappedSubAllChangePoints[\"Date\"] = pd.to_datetime(mappedSubAllChangePoints[\"Date\"], format = \"%Y/%m/%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols_orig=[\"Open_orig\",\"High_orig\",\"Low_orig\",\"Close_orig\", \"Volume_orig\"]\n",
    "if logarithm: #для моделей, где использована разность логарифмов, а не частное\n",
    "    mappedSubAllChangePoints[num_cols_orig]=np.exp(mappedSubAllChangePoints[[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]])\n",
    "else:    \n",
    "    mappedSubAllChangePoints[num_cols_orig]=mappedSubAllChangePoints[\"Open\",\"High\",\"Low\",\"Close\", \"Volume\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_cols=[\"Date\",\"Stockname\"]\n",
    "relevant_cols.extend(num_cols)\n",
    "relevant_cols.extend(['ScoredNewTrigger_tmp',\n",
    "                       'ScoredNewTypeBool',\n",
    "                       'ScoredNewTrigger',\n",
    "                       'ScoredNewIDselect'])\n",
    "if mode==\"OUT\": relevant_cols.extend(['NewTypeBool_Avg','NewTrigger_Avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappedSubAllChangePoints.dropna(subset=relevant_cols,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поскольку эксперт не разделял тренды на восходящие и нисходящие,\n",
    "# то и на графике будет два типа истинных значения: тренд или флэт\n",
    "if mode==\"OUT\":\n",
    "    mappedSubAllChangePoints.loc[mappedSubAllChangePoints['NewTypeBool_Avg'] == -1, 'NewTypeBool_Avg'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (136 of 136) |#######################| Elapsed Time: 0:02:52 Time: 0:02:52\n"
     ]
    }
   ],
   "source": [
    "# Строится отдельный график для каждой акции\n",
    "allstocks=list(mappedSubAllChangePoints['Stockname'].drop_duplicates())\n",
    "bar = pbar.ProgressBar(max_value=len(allstocks)).start() # Создаём новый progress bar\n",
    "t=0\n",
    "for stock_name in allstocks:\n",
    "    # Выбираются из всех данных только данные по одной акции\n",
    "    stock_data = mappedSubAllChangePoints[mappedSubAllChangePoints['Stockname'] == stock_name].copy()\n",
    "    # Сортировка всех данных по дате\n",
    "    stock_data.sort_values('Date', ascending=True, inplace=True)\n",
    "    \n",
    "    # Название акции без технических слов/символов\n",
    "    name = stock_name.split(' ')[0].split('_')[0]\n",
    "    \n",
    "    # Линия акции\n",
    "    trace_stock = py.graph_objs.Candlestick(x=stock_data['Date'],\n",
    "                                            open=stock_data['Open_orig'] ,\n",
    "                                            high=stock_data['High_orig'] ,\n",
    "                                            low=stock_data['Low_orig'],\n",
    "                                            close=stock_data['Close_orig'],\n",
    "                                            name=name)\n",
    "    \n",
    "    # Линия объема акции\n",
    "    trace_volume = py.graph_objs.Scatter(x=stock_data['Date'],\n",
    "                                         y=stock_data['Volume_orig'],\n",
    "                                         mode='lines',\n",
    "                                         name='Volume',\n",
    "                                         line=dict(color='#7C2209',\n",
    "                                                   width=3),\n",
    "                                         yaxis='y2')\n",
    "    # В случае режима работы с размеченными данными создается фоновая подложка истинных значений тренд/флэт\n",
    "    if mode==\"OUT\":\n",
    "        stock_data['tmp'] = stock_data['NewTypeBool_Avg'].diff()\n",
    "        stock_data.set_value(stock_data.index[0], 'tmp', 1)\n",
    "        stock_data.set_value(stock_data.index[-1], 'tmp', 1)\n",
    "    \n",
    "        trend_first = stock_data.loc[stock_data.index[0], 'NewTypeBool_Avg'] == 1\n",
    "    color_trend = '#54d88c'  # Цвет восходящего тренда\n",
    "    color_flat = '#ff4633'  # Цвет нисходящего тренда\n",
    "    \n",
    "    if mode==\"OUT\":\n",
    "        dates = stock_data['Date'][stock_data['tmp'] != 0]\n",
    "        shapes = [{'type': 'rect',\n",
    "                        'xref': 'x',\n",
    "                        'yref': 'paper',\n",
    "                        'x0': i - timedelta(days=1),\n",
    "                        'y0': 0,\n",
    "                        'x1': j - timedelta(days=1),\n",
    "                        'y1': 1,\n",
    "                        'fillcolor': color_trend if trend_first else color_flat,\n",
    "                        'opacity': 0.25,\n",
    "                        'line': dict(width=0)} for i, j in zip(dates[::2], dates[1::2])]\n",
    "\n",
    "        shapes.extend([{'type': 'rect',\n",
    "                             'xref': 'x',\n",
    "                             'yref': 'paper',\n",
    "                             'x0': i - timedelta(days=1),\n",
    "                             'y0': 0,\n",
    "                             'x1': j - timedelta(days=1),\n",
    "                             'y1': 1,\n",
    "                             'fillcolor': color_flat if trend_first else color_trend,\n",
    "                             'opacity': 0.25,\n",
    "                             'line': dict(width=0)} for i, j in zip(dates[1::2], dates[2::2])])\n",
    "\n",
    "        # Фиктивная (невидимая) точка для создания элемента легенды,\n",
    "        # отвечающего за истинное значение флэта, размеченного экспертом\n",
    "        # (подложка не создает элемент легенды)\n",
    "        trace_flat_expert = py.graph_objs.Scatter(x=[stock_data['Date'][stock_data.index[0]]],\n",
    "                                                      y=[stock_data['Close_orig'][stock_data.index[0]]],\n",
    "                                                       hoverinfo='none',\n",
    "                                                       visible='legendonly',\n",
    "                                                       name='Flat - Ground Truth',\n",
    "                                                       mode='markers',\n",
    "                                                       marker=dict(size=20,\n",
    "                                                                   color='#ff4633',\n",
    "                                                                   opacity=0.25,\n",
    "                                                                   symbol='square',\n",
    "                                                                   line = dict(width=0)))\n",
    "\n",
    "        # Фиктивная (невидимая) точка для создания элемента легенды,\n",
    "        # отвечающего за истинное значение тренда, размеченного экспертом\n",
    "        # (подложка не создает элемент легенды)\n",
    "        trace_trend_expert = py.graph_objs.Scatter(x=[stock_data['Date'][stock_data.index[0]]],\n",
    "                                                        y=[stock_data['Close_orig'][stock_data.index[0]]],\n",
    "                                                        hoverinfo='none',\n",
    "                                                        visible='legendonly',\n",
    "                                                        name='Trend - Ground Truth',\n",
    "                                                        mode='markers',\n",
    "                                                        marker=dict(size=20,\n",
    "                                                                    color='#54d88c',\n",
    "                                                                    opacity=0.25,\n",
    "                                                                    symbol='square',\n",
    "                                                                    line = dict(width=0)))\n",
    "\n",
    "        # Вертикальные черные штриховые линии, обозначающие точки перегиба, отмеченные экспертом\n",
    "        cp_gt = [py.graph_objs.Scatter(x=[dt, dt],\n",
    "                                            y=[trace_stock['low'].min(), trace_stock['high'].max()],\n",
    "                                            mode='lines',\n",
    "                                            legendgroup='ChangePointsGT',\n",
    "                                            showlegend=False,\n",
    "                                            hoverinfo='none',\n",
    "                                            line=dict(dash='dash',\n",
    "                                                      width=1,\n",
    "                                                      color='black')) for dt in stock_data['Date'][stock_data['NewTrigger_Avg'] == 1]]\n",
    "        if cp_gt:\n",
    "            cp_gt[0]['showlegend'] = True\n",
    "            cp_gt[0]['name'] = 'ChangePoints - Ground truth'\n",
    "\n",
    "        stock_data.dropna(subset=['ScoredNewTrigger_tmp',\n",
    "                                       'ScoredNewTypeBool',\n",
    "                                       'ScoredNewTrigger',\n",
    "                                       'ScoredNewIDselect'], inplace=True)\n",
    "\n",
    "    stock_data.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    # Вертикальные черные сплошные линии, обозначающие точки перегиба, отмеченные моделью\n",
    "    cp_model = [py.graph_objs.Scatter(x=[dt, dt],\n",
    "                                      y=[trace_stock['low'].min(), trace_stock['high'].max()],\n",
    "                                      mode='lines',\n",
    "                                      legendgroup='ChangePointsModel',\n",
    "                                      showlegend=False,\n",
    "                                      hoverinfo='none',\n",
    "                                      line=dict(width=1,\n",
    "                                                color='black')) for dt in stock_data['Date'][stock_data['ScoredNewTrigger'] == 1]]\n",
    "    if cp_model:\n",
    "        cp_model[0]['showlegend'] = True\n",
    "        cp_model[0]['name'] = 'ChangePoints - Model'\n",
    "        \n",
    "    # Вертикальные красные штриховые линии, обозначающие точки,\n",
    "    # в которых модель получила сигнал (гипотетические точки перегиба)\n",
    "    cp_signal = [py.graph_objs.Scatter(x=[dt, dt],\n",
    "                                       y=[trace_stock['low'].min(), trace_stock['high'].max()],\n",
    "                                       mode='lines',\n",
    "                                       legendgroup='ChangePointsSignal',\n",
    "                                       showlegend=False,\n",
    "                                       hoverinfo='none',\n",
    "                                       line=dict(dash='dash',\n",
    "                                                 width=1,\n",
    "                                                 color='red')) for dt in stock_data['Date'][stock_data['ScoredNewTrigger_tmp'] == 1]]\n",
    "    if cp_signal:\n",
    "        cp_signal[0]['showlegend'] = True\n",
    "        cp_signal[0]['name'] = 'ChangePoints - Signal'\n",
    "\n",
    "    # Создаются три массива дат, в которых модель видит: -1: нисходящий тренд,\n",
    "    #                                                     0: флэт,\n",
    "    #                                                     1: восходящий тренд\n",
    "    stock_data.loc[stock_data.index[-1], 'ScoredNewTypeBool'] = -3\n",
    "    prev_phase = stock_data.loc[stock_data.index[0], 'ScoredNewTypeBool']\n",
    "    start = stock_data.index[0]\n",
    "    phases = {-1:[], 0:[], 1:[]}\n",
    "    for row_id, _ in stock_data.iterrows():\n",
    "        cur_phase = stock_data.loc[row_id, 'ScoredNewTypeBool']\n",
    "        if cur_phase != prev_phase:\n",
    "            phases[prev_phase].append((stock_data.loc[start, 'Date'], stock_data.loc[row_id, 'Date']))\n",
    "            start = row_id\n",
    "        prev_phase = cur_phase\n",
    "    \n",
    "    colors = {-1: color_flat, 0: '#3f51b5', 1: color_trend}\n",
    "    yys = {-1: trace_stock['high'].max(), 0: trace_stock['low'].min(), 1: trace_stock['high'].max()}\n",
    "    names = {-1: 'Decreasing Trend', 0: 'Flat', 1: 'Increasing Trend'}\n",
    "\n",
    "    # Линия нисходящих трендов, размеченных моделью\n",
    "    trace_trend_dec = [py.graph_objs.Scatter(x=[i, j],\n",
    "                                             y=[yys[-1],\n",
    "                                                yys[-1]],\n",
    "                                             legendgroup='TrendOrFlatModel1',\n",
    "                                             showlegend=False,\n",
    "                                             hoverinfo='none',\n",
    "                                             mode='lines',\n",
    "                                             line=dict(color=colors[-1],\n",
    "                                                       width=5)) for i, j in phases[-1]]\n",
    "    if trace_trend_dec:\n",
    "        trace_trend_dec[0]['showlegend'] = True\n",
    "        trace_trend_dec[0]['name'] = '{0} - Model'.format(names[-1])\n",
    "        \n",
    "    # Линия восходящих трендов, размеченных моделью\n",
    "    trace_trend_inc = [py.graph_objs.Scatter(x=[i, j],\n",
    "                                             y=[yys[1],\n",
    "                                                yys[1]],\n",
    "                                             legendgroup='TrendOrFlatModel2',\n",
    "                                             showlegend=False,\n",
    "                                             hoverinfo='none',\n",
    "                                             mode='lines',\n",
    "                                             line=dict(color=colors[1],\n",
    "                                                       width=5)) for i, j in phases[1]]\n",
    "    \n",
    "    if trace_trend_inc:\n",
    "        trace_trend_inc[0]['showlegend'] = True\n",
    "        trace_trend_inc[0]['name'] = '{0} - Model'.format(names[1])\n",
    "        \n",
    "    # Линия флэтов, размеченных моделью\n",
    "    trace_flat = [py.graph_objs.Scatter(x=[i, j],\n",
    "                                        y=[yys[0],\n",
    "                                           yys[0]],\n",
    "                                        legendgroup='TrendOrFlatModel3',\n",
    "                                        showlegend=False,\n",
    "                                        hoverinfo='none',\n",
    "                                        mode='lines',\n",
    "                                        line=dict(color=colors[0],\n",
    "                                                  width=5)) for i, j in phases[0]]\n",
    "    \n",
    "    if trace_flat:\n",
    "        trace_flat[0]['showlegend'] = True\n",
    "        trace_flat[0]['name'] = '{0} - Model'.format(names[0])\n",
    "    \n",
    "    # Случай режима работы с размеченными данными (данные содержат разметку эксперта)\n",
    "    if mode==\"OUT\":\n",
    "        \n",
    "        # Все линии, относящиеся к разметке, произведенной экспертом и моделью помещаются в массив\n",
    "        data = py.graph_objs.Data([trace_stock,\n",
    "                               trace_volume,\n",
    "                               *cp_gt,\n",
    "                               *cp_model,\n",
    "                               *cp_signal,\n",
    "                               trace_flat_expert,\n",
    "                               trace_trend_expert,\n",
    "                               *trace_trend_dec,\n",
    "                               *trace_trend_inc,\n",
    "                               *trace_flat])\n",
    "\n",
    "        # Создается объект фона графика, на котором присутствует фоновая подложка истинных значений тренд/флэт\n",
    "        layout = py.graph_objs.Layout(title=name,\n",
    "                                  xaxis=dict(autorange=True,\n",
    "                                             rangeslider=dict(visible=False)),\n",
    "                                  yaxis2=dict(overlaying='y',\n",
    "                                              side='right',\n",
    "                                              zeroline=False,\n",
    "                                              showgrid=False,),\n",
    "                                  autosize=True,\n",
    "                                  shapes=shapes)\n",
    "\n",
    "    # Случай режима работы без наличия истинной разметки эксперта\n",
    "    else:\n",
    "        # Все линии, относящиеся к разметке, произведенной только моделью помещаются в массив\n",
    "        data = py.graph_objs.Data([trace_stock,\n",
    "                               trace_volume,\n",
    "                               #*cp_gt,\n",
    "                               *cp_model,\n",
    "                               *cp_signal,\n",
    "                               #trace_flat_expert,\n",
    "                               #trace_trend_expert,\n",
    "                               *trace_trend_dec,\n",
    "                               *trace_trend_inc,\n",
    "                               *trace_flat])\n",
    "\n",
    "        # Создается объект фона графика, на котором отсутствует фоновая подложка истинных значений тренд/флэт\n",
    "        layout = py.graph_objs.Layout(title=name,\n",
    "                                  xaxis=dict(autorange=True,\n",
    "                                             rangeslider=dict(visible=False)),\n",
    "                                  yaxis2=dict(overlaying='y',\n",
    "                                              side='right',\n",
    "                                              zeroline=False,\n",
    "                                              showgrid=False,),\n",
    "                                  autosize=True)#,\n",
    "                                  #shapes=shapes)\n",
    "            \n",
    "    # Все необходимые линии наносятся на график, а также отрисовывается фон\n",
    "    fig = py.graph_objs.Figure(data=data, layout=layout)\n",
    "\n",
    "    # График сохраняется в формате html в папку \"Pictures\" и имеет название соответствующей акции\n",
    "    py.offline.plot(fig, auto_open=False, filename=os.path.join(path,\"Pictures\",pipe_file_name+'_{0}.html').format(name))\n",
    "    t+=1\n",
    "    bar.update(t)\n",
    "bar.finish()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
